{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF8L0RG4F1gz"
      },
      "source": [
        "# Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5946MrAFmxb"
      },
      "source": [
        "!pip install seaborn\n",
        "!pip install causalgraphicalmodels\n",
        "!pip install matplotlib\n",
        "!pip install -U scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "import causalgraphicalmodels\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg0KrKDqGBp5"
      },
      "source": [
        "#  MSBD model building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxwUsautGMth"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "def epsilon_z1():\n",
        "    return np.random.normal(loc = 1,scale = 0.5)\n",
        "def epsilon_x1():\n",
        "    return np.random.normal(loc = 0,scale = 1)\n",
        "def epsilon_y1():\n",
        "    return np.random.normal(loc = 0,scale = 1)\n",
        "def epsilon_z2():\n",
        "    return np.random.normal(loc = 0,scale = 0.5)\n",
        "def epsilon_x2():\n",
        "    return np.random.normal(loc = 0,scale = 1)\n",
        "def epsilon_y2():\n",
        "    return np.random.normal(loc = 0,scale = 0.5)\n",
        "def para_c_a1():\n",
        "    return np.random.normal(loc=2,scale=1)\n",
        "def para_c_a2():\n",
        "    return np.random.normal(loc=-2,scale=1)\n",
        "def para_c_x1():\n",
        "    return np.random.normal(loc=-1,scale=0.5)\n",
        "def para_c_y1():\n",
        "    return np.random.normal(loc=1,scale=0.8)\n",
        "def para_c_b1():\n",
        "    return np.random.normal(loc=1,scale=0.5)\n",
        "def para_c_b2():\n",
        "    return np.random.normal(loc=-1,scale=1)\n",
        "def para_cxb1():\n",
        "    return np.random.normal(loc=0.3,scale=1)\n",
        "def para_cxb2():\n",
        "    return np.random.normal(loc=0.2,scale=1)\n",
        "def para_cyb1():\n",
        "    return np.random.normal(loc=0.3,scale=1)\n",
        "def para_cyb2():\n",
        "    return np.random.normal(loc = -0.5,scale=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-YPv4sNGSVK"
      },
      "source": [
        "\n",
        "def build_z1():\n",
        "    return sigmoid(para_c_a1() + para_c_a2() *epsilon_z1() + epsilon_z1())\n",
        "def build_x1(z_sum):\n",
        "    return sigmoid(0.2*z_sum + epsilon_x1())\n",
        "def build_y1(z1,x1,d):\n",
        "    sum = 0.0\n",
        "    for i in range(d):\n",
        "        sum += z1[i] * para_c_y1()\n",
        "    return sigmoid(sum - (2*x1 - 1) - epsilon_y1() - 2)\n",
        "def build_x2(z1,x1,y1,z2,d):\n",
        "    sum = 0.0\n",
        "    sum_1 = 0.0\n",
        "    for i in range(d):\n",
        "        sum += z1[i]*para_cxb1()\n",
        "        sum_1 += z2[i]*para_cxb2()\n",
        "    return sigmoid(-sum + sum_1 + 2*x1 - 2 +y1 +epsilon_x2())\n",
        "def build_y2(z1,x1,y1,z2,x2,d):\n",
        "    sum = 0.0\n",
        "    sum_1 = 0.0\n",
        "    for i in range(d):\n",
        "        sum += z1[i]*para_cyb1()\n",
        "        sum_1 += z2[i]*para_cyb2()\n",
        "    return sigmoid(-0.5*(sum + sum_1 + epsilon_y2() - 1) + 2*x1 + 2*x2-2 -epsilon_y2()+y1 -epsilon_y2())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raB6GIFnGXr4"
      },
      "source": [
        "def build_causal_model(N,d):\n",
        "    scm_np = np.empty((N,2*d+4))\n",
        "    col_name = list()\n",
        "    sum = 0.0\n",
        "    for i in range(N):\n",
        "        z1_list = list()\n",
        "        z2_list = list()\n",
        "        for j in range(d):\n",
        "            if(i == 0):\n",
        "                col_name.append('z1,'+str(j))\n",
        "            z_1 = np.random.binomial(n=1, p=build_z1())\n",
        "            scm_np[i][j] = z_1\n",
        "            sum += z_1 * para_c_x1()\n",
        "            z1_list.append(z_1)\n",
        "        if i==0:\n",
        "            col_name.extend(['x1','y1'])\n",
        "        x_1 = np.random.binomial(n=1, p=build_x1(sum))\n",
        "        scm_np[i][d] = x_1\n",
        "        y_1 = np.random.binomial(n=1,p=build_y1(z1_list,x_1,d))\n",
        "        scm_np[i][d+1] = y_1\n",
        "        sum = 0.0\n",
        "        for j in range(0,d):\n",
        "            if(i == 0):\n",
        "                col_name.append('z2,'+str(j))\n",
        "            z_2 = np.random.binomial(n=1, p = sigmoid(z1_list[j]*para_c_b1() + (2*x_1-1)+epsilon_z2()+y_1-para_c_b2()))\n",
        "            scm_np[i][d+2+j] = z_2\n",
        "            z2_list.append(z_2)\n",
        "        if i == 0:\n",
        "            col_name.extend(['x2','y2'])\n",
        "        x_2 =np.random.binomial(n=1,p= build_x2(z1_list,x_1, y_1,z2_list,d))\n",
        "        scm_np[i][2*d+2] = x_2\n",
        "        y_2 = build_y2(z1_list,x_1,y_1,z2_list,x_2,d)\n",
        "        scm_np[i][2*d+3] = y_2\n",
        "    return scm_np,col_name\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5G_2qJHDtH"
      },
      "source": [
        "# Do Operator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klmr4JiGgNA"
      },
      "source": [
        "def causal_do(scm,new_x1,new_x2,d):\n",
        "    scm_copy = scm.copy()\n",
        "    scm_copy[:,d] = new_x1\n",
        "    scm_copy[:,2*d+2] = new_x2\n",
        "    size = scm_copy.shape[0]\n",
        "    z1_list = list()\n",
        "    z2_list = list()\n",
        "    for i in range(size):\n",
        "        z1_list = scm_copy[i,0:d].tolist()\n",
        "        y_1 = np.random.binomial(n=1,p=build_y1(z1_list,new_x1[i],d))\n",
        "        scm_copy[i][d+1] = y_1\n",
        "        for j in range(0,d):\n",
        "            z_2 = np.random.binomial(n=1, p = sigmoid(z1_list[j]*para_c_b1() + (2*new_x1[i]-1)+epsilon_z2()+y_1-para_c_b2()))\n",
        "            scm_copy[i][d+2+j] = z_2\n",
        "            z2_list.append(z_2)\n",
        "        y_2 = build_y2(z1_list,new_x1[i],y_1,z2_list,new_x2[i],d)\n",
        "        scm_copy[i][2*d+3] = y_2\n",
        "\n",
        "    return scm_copy\n",
        "\n",
        "def estimate_conditional_expectation1(df,d,l_1,l_2):\n",
        "    scm_cols = np.size(df,axis=1)\n",
        "    scm_rows = np.size(df,axis=0)\n",
        "    # result_arr = []\n",
        "    c = df[:,d] == l_1[0]\n",
        "    d = df[:,d] == l_1[1]\n",
        "    a = df[:,scm_cols-2] == l_2[0]\n",
        "    b = df[:,scm_cols-2] == l_2[1]\n",
        "    z_z = np.zeros(scm_rows)\n",
        "    z_z = z_z+a+c == 2\n",
        "    #print(e)\n",
        "    o_z = np.zeros(scm_rows)\n",
        "    o_z = o_z+d+b == 2\n",
        "    y = df[:,scm_cols-1]\n",
        "\n",
        "    p = np.mean(y[z_z])\n",
        "    q = np.mean(y[o_z])\n",
        "    return [p,q]\n",
        "\n",
        "def ab_test1(scm, n , d):\n",
        "    n_a = int(n / 2)\n",
        "    n_b = n - n_a\n",
        "    result_dic = dict()\n",
        "    result_arr = []\n",
        "    result_temp = []\n",
        "    set_variable_1 = np.array([0]*n_a + [1]*n_b)\n",
        "    j=0\n",
        "    set_variable_2 = np.array([0]*n_a + [0]*n_b)\n",
        "    scm = causal_do(scm,set_variable_1,set_variable_2,d)\n",
        "    #print(string)\n",
        "    result_arr = estimate_conditional_expectation1(scm,d,[0,1],[0,0])\n",
        "    set_variable_1 = np.array([0]*n_a + [1]*n_b)\n",
        "    j=0\n",
        "    set_variable_2 = np.array([1]*n_a + [1]*n_b)\n",
        "    scm = causal_do(scm,set_variable_1,set_variable_2,d)\n",
        "\n",
        "    result_temp = estimate_conditional_expectation1(scm,d, [0,1],[1,1])\n",
        "    result_arr.extend(result_temp)\n",
        "\n",
        "    print(result_arr)\n",
        "    return result_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6z0OHXHjto"
      },
      "source": [
        "# Weighted Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEIq8KNMHNvd"
      },
      "source": [
        "def p_x(fd,s):\n",
        "    df_copy = fd[s].copy()\n",
        "    size = fd.shape[0]\n",
        "    pw2 = np.zeros(2)\n",
        "    prob_w2 = np.zeros(size)\n",
        "    for i in [0,1]:\n",
        "        nm1 = df_copy == i\n",
        "        pw2[i] = nm1.mean()\n",
        "    for i in range(size):\n",
        "        if df_copy[i] == 0:\n",
        "            prob_w2[i] = pw2[0]\n",
        "        else :\n",
        "            prob_w2[i] = pw2[1]\n",
        "    return prob_w2\n",
        "\n",
        "def p_x2_x1(fd):\n",
        "    df_copy = fd.copy()\n",
        "    proba_x2_x1 = np.ones(fd.shape[0])\n",
        "    X = df_copy['x1'].values.reshape(-1,1)\n",
        "    y = df_copy['x2']\n",
        "    clf = LogisticRegression(random_state = 0).fit(X,y)\n",
        "    predict = clf.predict_proba(X)\n",
        "    for j in range(fd.shape[0]):\n",
        "        if( y[j] == 0):\n",
        "            proba_x2_x1[j] = predict[j,0]\n",
        "        else :\n",
        "            proba_x2_x1[j] = predict[j,1]\n",
        "    return proba_x2_x1\n",
        "\n",
        "def p_x1_z1(fd):\n",
        "    df_copy = fd.copy()\n",
        "    proba_x1_z1 = np.ones(fd.shape[0])\n",
        "    z1 = df_copy.iloc[:,0:d]\n",
        "    X = z1.values.reshape(-1,d)\n",
        "    y = df_copy['x1']\n",
        "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
        "    predict = clf.predict_proba(X)\n",
        "    for j in range(fd.shape[0]):\n",
        "        if( y[j] == 0):\n",
        "            proba_x1_z1[j] = predict[j,0]\n",
        "        else :\n",
        "            proba_x1_z1[j] = predict[j,1]\n",
        "    return proba_x1_z1\n",
        "\n",
        "def p_x2_oth(fd):\n",
        "    df_copy = fd.copy()\n",
        "    y = df_copy['x2']\n",
        "    df_copy = df_copy.drop(columns = ['x2','y2'])\n",
        "    d = df_copy.shape[1]\n",
        "    X = df_copy.values.reshape(-1,d)\n",
        "    proba_x2 = np.ones(fd.shape[0])\n",
        "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
        "    predict = clf.predict_proba(X)\n",
        "    for j in range(fd.shape[0]):\n",
        "        if( y[j] == 0):\n",
        "            proba_x2[j] = predict[j,0]\n",
        "        else :\n",
        "            proba_x2[j] = predict[j,1]\n",
        "    return proba_x2\n",
        "\n",
        "\n",
        "def weightGenerator(df):\n",
        "   data1 = np.multiply(p_x(df,'x1'),p_x(df,'x2'))\n",
        "   data2 = np.multiply(p_x1_z1(df),p_x2_oth(df))\n",
        "   data_prob = np.divide(data1,data2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTUM8kurv41t"
      },
      "source": [
        "# CWO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Zy17U_Hvpr"
      },
      "source": [
        "\n",
        "def cwoBeta(X,y,weight):\n",
        "  Beta1 = LinearRegression().fit(X,y,sample_weight=weight)\n",
        "  test_x=np.array([[0,0],[1,0],[0,1],[1,1]]).reshape((-1, 2))\n",
        "\n",
        "  y_pred=Beta1.predict(test_x)\n",
        "\n",
        "  return y_pred\n",
        "def weightedATE(df):\n",
        "    data = df.copy()\n",
        "    X = data[['x1','x2']].values.reshape(-1,2)\n",
        "    y = data['y2'].values.reshape(-1,1)\n",
        "    weight = weightGenerator(df)\n",
        "    ate = cwoBeta(X,y,weight)\n",
        "    #print(ate)\n",
        "    return ate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3UVF4tNY1LL"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8htzojsOTot"
      },
      "source": [
        "def nnBeta(X,y,testX,weight,hp):\n",
        "  Beta2 = tf.keras.models.Sequential()\n",
        "  Beta2.add(tf.keras.layers.Dense(hp['input_units'],activation='relu',input_shape=(2,)))\n",
        "  Beta2.add(tf.keras.layers.Dropout(hp['dropout']))\n",
        "  for i in range(hp['n_layers']):\n",
        "    Beta2.add(tf.keras.layers.Dense(hp[f'conv_{i}_units'], activation='relu'))\n",
        "    Beta2.add(tf.keras.layers.Dropout(hp[f'dropout_{i}_']))\n",
        "\n",
        "  Beta2.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "  Beta2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']), loss='mse')\n",
        "\n",
        "  early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=40, verbose=0, mode='auto', restore_best_weights=True)\n",
        "  history=Beta2.fit(X,y,epochs=1000,\n",
        "                    validation_split=0.2,\n",
        "                    shuffle = True,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=0\n",
        "                    ,sample_weight=weight\n",
        "                    )\n",
        "\n",
        "\n",
        "  y_pred = Beta2.predict(testX)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9DHgZpoI8Le"
      },
      "source": [
        "def NeuralATE(df,hp):\n",
        "  X = df[['x1','x2']].values.reshape(-1,2)\n",
        "  y = df['y2'].values.reshape(-1,1)\n",
        "  sampleWeight = weightGenerator(df)\n",
        "  test=np.array([[0,0],[1,0],[0,1],[1,1]]).reshape((-1, 2))\n",
        "  test_x=pd.DataFrame(test,columns=['a','b'])\n",
        "  XX=test_x.values.reshape(-1, 2)\n",
        "  ate = nnBeta(X,y,XX,sampleWeight,hp)\n",
        "\n",
        "\n",
        "  return ate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWkfDZcQW5Hl"
      },
      "source": [
        "# Result Storing in Pickle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evyC72USO8Lu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8SH7FJuS-F5"
      },
      "source": [
        "d=7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTZwWd-IGHaB"
      },
      "source": [
        "def doDo(d):\n",
        "  N=10000000\n",
        "  fdDo,col_name=build_causal_model(N,d)\n",
        "  print(fdDo)\n",
        "  mu0,mu1,mu2,mu3=ab_test1(fdDo, N,d)\n",
        "  df = pd.DataFrame(fdDo,columns = col_name)\n",
        "  return df,mu0,mu1,mu2,mu3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmkcMH5hrESG"
      },
      "source": [
        "fd,mu0,mu1,mu2,mu3=doDo(d)\n",
        "print(\"Estimated ATE: {:.3f}\".format(mu1-mu0))\n",
        "print(\"mu(1): {:.3f}\".format(mu1))\n",
        "print(\"mu(0): {:.3f}\".format(mu0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDmPMn1mwZBQ"
      },
      "source": [
        "fd.to_pickle(\"drive/MyDrive/Datasets/msbd_dataframe_7.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWYymiikw9Zv"
      },
      "source": [
        "string1 = str(mu0) + \" \" + str(mu1) + \" \" + str(mu2) + \" \" + str(mu3)\n",
        "print(string1)\n",
        "f = open(\"drive/MyDrive/Datasets/msbd_mu_5.txt\",\"w\")\n",
        "f.write(string1)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIRczfo_Zy1f"
      },
      "source": [
        "# Pickle data extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q3rhZ6unYKF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grZKHoenx-Rt"
      },
      "source": [
        "d=8\n",
        "fd = pd.read_pickle(\"drive/MyDrive/Datasets/msbd_dataframe_88.pkl\")\n",
        "print(fd)\n",
        "f = open('drive/MyDrive/Datasets/msbd_mu_88.txt','r')\n",
        "mm = f.read()\n",
        "mu=np.ones(4)\n",
        "mu0,mu1,mu2,mu3 = mm.split()\n",
        "mu[1]=float(mu1)\n",
        "mu[0]=float(mu0)\n",
        "mu[2]=float(mu2)\n",
        "mu[3]=float(mu3)\n",
        "print(mu0,mu1,mu2,mu3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QsB3wp3HyKn"
      },
      "source": [
        "# Result Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1UirzssDHW_"
      },
      "source": [
        "def MAAEgenerator(fd,mu):\n",
        "\n",
        "  muN=NeuralATE(fd,hpDict)\n",
        "\n",
        "  muC=weightedATE(fd)\n",
        "  maaeN=(abs(muN[0]-mu[0])+abs(muN[1]-mu[1])+abs(muN[2]-mu[2])+abs(muN[3]-mu[3]))/4\n",
        "  maaeC=(abs(muC[0]-mu[0])+abs(muC[1]-mu[1])+abs(muC[2]-mu[2])+abs(muC[3]-mu[3]))/4\n",
        "  return maaeN,maaeC\n",
        "\n",
        "#print(MAAEgenerator(fd,mu1,mu0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV6egAHvfUJ7",
        "outputId": "7435fc86-65d0-4fea-e947-5a0fdb841210"
      },
      "source": [
        "for N_SAMPLE in range(500,501,500):\n",
        "  muN=np.zeros(100)\n",
        "  muC=np.zeros(100)\n",
        "  for k in range(100):\n",
        "    # print(\"K= \",k)\n",
        "    db=fd.sample(n=N_SAMPLE).reset_index(drop=True)\n",
        "    muN[k],muC[k]=MAAEgenerator(db,mu)\n",
        "    # print(k,muN[k],muC[k])\n",
        "  MAAE_N=np.median(muN)\n",
        "  print(\"sample size\" , N_SAMPLE)\n",
        "  print(\"MAAE NN\",MAAE_N)\n",
        "  MAAE_C=np.median(muC)\n",
        "  print(\"MAAE CWO\",MAAE_C)\n",
        "  print('loooooolllllll')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample size 500\n",
            "MAAE NN 0.03483150899410248\n",
            "MAAE CWO 0.03419560898187281\n",
            "loooooolllllll\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wNE9trOnMbx"
      },
      "source": [
        "for N_SAMPLE in range(10000,10001,500):\n",
        "  muN=np.zeros(100)\n",
        "  muC=np.zeros(100)\n",
        "  for k in range(100):\n",
        "    # print(\"K= \",k)\n",
        "    db=fd.sample(n=N_SAMPLE).reset_index(drop=True)\n",
        "    muN[k],muC[k]=MAAEgenerator(db,mu)\n",
        "    # print(k,muN[k],muC[k])\n",
        "  MAAE_N=np.median(muN)\n",
        "  print(\"sample size\" , N_SAMPLE)\n",
        "  print(\"MAAE NN\",MAAE_N)\n",
        "  MAAE_C=np.median(muC)\n",
        "  print(\"MAAE CWO\",MAAE_C)\n",
        "  print('loooooolllllll')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqQt5CkarYeD"
      },
      "source": [
        "for N_SAMPLE in range(7500,10001,500):\n",
        "  muN=np.zeros(100)\n",
        "  muC=np.zeros(100)\n",
        "  for k in range(100):\n",
        "    # print(\"K= \",k)\n",
        "    db=fd.sample(n=N_SAMPLE).reset_index(drop=True)\n",
        "    muN[k],muC[k]=MAAEgenerator(db,mu)\n",
        "    # print(k,muN[k],muC[k])\n",
        "  MAAE_N=np.median(muN)\n",
        "  print(\"sample size\" , N_SAMPLE)\n",
        "  print(\"MAAE NN\",MAAE_N)\n",
        "  MAAE_C=np.median(muC)\n",
        "  print(\"MAAE CWO\",MAAE_C)\n",
        "  print('loooooolllllll')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2Vj58qdF5e"
      },
      "source": [
        "# Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvvpE3TvdaDx"
      },
      "source": [
        "!pip install keras-tuner==1.0.0\n",
        "import kerastuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFNFieZeeR3K"
      },
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "import time\n",
        "LOG_DIR = f\"{int(time.time())}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ADZkzRdFR6"
      },
      "source": [
        "def tempNN(hp):\n",
        "  Beta2 = tf.keras.models.Sequential()\n",
        "  Beta2.add(tf.keras.layers.Dense(hp.Int('input_units',\n",
        "                                         min_value=10,\n",
        "                                         max_value=100,\n",
        "                                         step=10), activation='relu',input_shape=(2,)))\n",
        "  Beta2.add(tf.keras.layers.Dropout(hp.Float('dropout',\n",
        "                                             min_value=0.0,\n",
        "                                             max_value=0.4,\n",
        "                                             default=0.1,\n",
        "                                             step=0.05,)))\n",
        "  for i in range(hp.Int('n_layers', min_value=0, max_value=4,\n",
        "                                         step=1)):\n",
        "    Beta2.add(tf.keras.layers.Dense(hp.Int(f'conv_{i}_units',\n",
        "                                           min_value=10,\n",
        "                                           max_value=100,\n",
        "                                           step=10), activation='relu'))\n",
        "    Beta2.add(tf.keras.layers.Dropout(hp.Float(f'dropout_{i}_',\n",
        "                                             min_value=0.0,\n",
        "                                             max_value=0.4,\n",
        "                                             default=0.1,\n",
        "                                             step=0.05,)))\n",
        "\n",
        "  Beta2.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "  Beta2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate',\n",
        "                                                                            min_value=1e-5,\n",
        "                                                                            max_value=1e-2,\n",
        "                                                                            sampling='LOG',\n",
        "                                                                            default=1e-3)), loss='mse')\n",
        "\n",
        "  return Beta2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9gCCsm3eYd5"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    tempNN,\n",
        "    objective='val_loss',\n",
        "    max_trials=5,  # how many model variations to test?\n",
        "    executions_per_trial=4,  # how many trials per variation? (same model could perform differently)\n",
        "    directory=LOG_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3zdY3P9XDhv"
      },
      "source": [
        "data=fd.sample(n=5000).reset_index(drop=True)\n",
        "X = data[['x1','x2']].values.reshape(-1,2)\n",
        "Y= data['y2'].values.reshape(-1,1)\n",
        "sampleWeight = weightGenerator(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20, verbose=1, mode='auto', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFF6LqAOr7TX",
        "outputId": "04688691-2cbb-4fbe-c291-2d6a580907e4"
      },
      "source": [
        "print(X.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIl4WAn6i384"
      },
      "source": [
        "tuner.search(X,Y,epochs=500,\n",
        "             validation_split=0.2,\n",
        "             shuffle = True,\n",
        "             callbacks=[early_stop],\n",
        "             verbose=0\n",
        "             ,sample_weight=sampleWeight\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiQVVtuP6OF7"
      },
      "source": [
        "tuner.get_best_hyperparameters()[0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nGDTxq_SQkh"
      },
      "source": [
        "hpDict=tuner.get_best_hyperparameters()[0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9jSY6qxSUu8"
      },
      "source": [
        "hpDict={'conv_0_units': 90,\n",
        " 'conv_1_units': 90,\n",
        " 'conv_2_units': 50,\n",
        " 'conv_3_units': 10,\n",
        " 'dropout': 0.2,\n",
        " 'dropout_0_': 0.05,\n",
        " 'dropout_1_': 0.1,\n",
        " 'dropout_2_': 0.35000000000000003,\n",
        " 'dropout_3_': 0.05,\n",
        " 'input_units': 90,\n",
        " 'learning_rate': 0.001338614394601499,\n",
        " 'n_layers': 4}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CD7fdSgUAjy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P3kwVe3DwMV"
      },
      "source": [
        "#for 8\n",
        "hpDict={'conv_0_units': 50,\n",
        " 'dropout': 0.05,\n",
        " 'dropout_0_': 0.15000000000000002,\n",
        " 'input_units': 40,\n",
        " 'learning_rate': 0.00503822561891769,\n",
        " 'n_layers': 1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJmBKyqXNAy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}